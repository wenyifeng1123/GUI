import os


import matplotlib.pyplot as plt
import numpy as np
import h5py

import keras.backend as K
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import SGD, Adam, RMSprop
from keras.utils import np_utils
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
from keras.models import load_model

# Load dataset

(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Only take a small part of the data to reduce computation time

X_train = X_train[:1000]
y_train = y_train[:1000]
X_test = X_test[:1000]
y_test = y_test[:1000]

# Define some variables from the dataset

nb_classes = np.unique(y_train).shape[0]
img_rows, img_cols = X_train.shape[-2:]

X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols).astype('float32')
X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols).astype('float32')

X_train /= 255
X_test /= 255

print('X_train shape:', X_train.shape)
print('y_train shape:', y_train.shape)

# Convert class vectors to binary class matrices

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

# Model parameters

nb_filters = 32
nb_pool = 2
kernel_size = (3, 3)

# Create the model

model = Sequential()

model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],
                        border_mode='valid', activation="relu",
                        input_shape=(1, img_rows, img_cols)))

model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],
                        border_mode='valid',activation="relu"))

model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.25))

model.add(Flatten())

# model.add(Dense(128))
# model.add(Activation('relu'))
# model.add(Dropout(0.5))

model.add(Dense(nb_classes))
model.add(Activation('softmax'))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(),
              metrics=['accuracy'])

batch_size = 128
nb_epoch = 5

history = model.fit(X_train, Y_train,
                    batch_size=batch_size, nb_epoch=nb_epoch,
                    verbose=1, validation_data=(X_test, Y_test))
'''


f=h5py.File('/no_backup/d1240/CNNArt/weights.h5','w')
weight_name='weights.h5'
model.save_weights(weight_name, overwrite=True)

'''
# Plotting functions

## make_mosaic(im, nrows, ncols, border=1)
## Create a numpy mosaic from a list of matrix.

## get_weights_mosaic(model, layer_id, n=64)
## Get the weights of the layer of a model as a numpy mosaic.

## plot_weights(model, layer_id, n=64, ax=None, **kwargs)
## Plot the weights of a specific layer with matplotlib

## plot_all_weights(model, n=64, **kwargs)
## Plot all the possible 2D weights in the model

## plot_feature_map(model, layer_id, X, n=256, ax=None, **kwargs)
## Plot the feature maps of the layer of a model

## plot_all_feature_maps(model, X, n=256, ax=None, **kwargs)
## Plot all the feature maps of every possible layers

def make_mosaic(im, nrows, ncols, border=1):
    """From http://nbviewer.jupyter.org/github/julienr/ipynb_playground/blob/master/keras/convmnist/keras_cnn_mnist.ipynb
    """
    import numpy.ma as ma

    nimgs = len(im)
    imshape = im[0].shape

    mosaic = ma.masked_all((nrows * imshape[0] + (nrows - 1) * border,
                            ncols * imshape[1] + (ncols - 1) * border),
                           dtype=np.float32)

    paddedh = imshape[0] + border
    paddedw = imshape[1] + border
    im
    for i in range(nimgs):
        row = int(np.floor(i / ncols))
        col = i % ncols

        mosaic[row * paddedh:row * paddedh + imshape[0],
        col * paddedw:col * paddedw + imshape[1]] = im[i]

    return mosaic


def get_weights_mosaic(model, layer_id, n):
    """
    """

    # Get Keras layer
    layer = model.layers[layer_id]

    # Check if this layer has weight values
    # the original way by using W is old Keras version, know should use the 'weights'
    if not hasattr(layer, "weights"):
        raise Exception("The layer {} of type {} does not have weights.".format(layer.name,
                                                                                layer.__class__.__name__))

    weights = layer.weights[0].container.data
    # the shape of the weight should be like (32,1,28,28) instead of (28,28,1,32)
    weights = np.transpose(weights, (3, 2, 0, 1))

    # For now we only handle Conv layer like with 4 dimensions
    if weights.ndim != 4:
        raise Exception("The layer {} has {} dimensions which is not supported.".format(layer.name, weights.ndim))

    # n define the maximum number of weights to display
    if weights.shape[0] < n:
        n = weights.shape[0]

    # Create the mosaic of weights
    nrows = int(np.round(np.sqrt(n)))
    ncols = int(nrows)

    if nrows ** 2 < n:
        ncols += 1

    mosaic = make_mosaic(weights[:n, 0], nrows, ncols, border=1)

    return mosaic


def plot_weights(model, layer_id, n, ax=None, **kwargs):
    """Plot the weights of a specific layer. ndim must be 4.
    """
    import matplotlib.pyplot as plt

    # Set default matplotlib parameters
    if not 'interpolation' in kwargs.keys():
        kwargs['interpolation'] = "none"

    if not 'cmap' in kwargs.keys():
        kwargs['cmap'] = "gray"

    layer = model.layers[layer_id]

    mosaic = get_weights_mosaic(model, layer_id, n=64)

    # Plot the mosaic
    if not ax:
        fig = plt.figure()
        ax = plt.subplot()

    im = ax.imshow(mosaic, **kwargs)
    ax.set_title("Layer #{} called '{}' of type {}".format(layer_id, layer.name, layer.__class__.__name__))

    plt.colorbar(im, ax=ax)

    return ax


def plot_all_weights(model, n=64, **kwargs):
    """
    """
    import matplotlib.pyplot as plt
    from mpl_toolkits.axes_grid1 import make_axes_locatable

    # Set default matplotlib parameters
    if not 'interpolation' in kwargs.keys():
        kwargs['interpolation'] = "none"

    if not 'cmap' in kwargs.keys():
        kwargs['cmap'] = "gray"

    layers_to_show = []
    
    # Here only choose the first 2 layers, since the other layers don't have weights, the 'weights' part of others are empty
    for i, layer in enumerate(model.layers[0:2]):
        if hasattr(layer, "weights"):
            weights = layer.weights[0].container.data
            if weights.ndim == 4:
                layers_to_show.append((i, layer))

    fig = plt.figure(figsize=(15, 15))

    n_mosaic = len(layers_to_show)
    #n_mosaic = len(model.layers)
    nrows = int(np.round(np.sqrt(n_mosaic)))
    ncols = int(nrows)

    if nrows ** 2 < n_mosaic:
        ncols += 1

    for i, (layer_id, layer) in enumerate(layers_to_show):
        mosaic = get_weights_mosaic(model, layer_id=layer_id, n=n)

        ax = fig.add_subplot(nrows, ncols, i + 1)

        im = ax.imshow(mosaic, **kwargs)
        ax.set_title("Layer #{} called '{}' of type {}".format(layer_id, layer.name, layer.__class__.__name__))

        divider = make_axes_locatable(ax)
        cax = divider.append_axes("right", size="5%", pad=0.1)
        plt.colorbar(im, cax=cax)

    fig.tight_layout()
    return fig


def plot_feature_map(model, layer_id, X, n=256, ax=None, **kwargs):
    """
    """
    import keras.backend as K
    import matplotlib.pyplot as plt
    from mpl_toolkits.axes_grid1 import make_axes_locatable

    layer = model.layers[layer_id]

    try:
        get_activations = K.function([model.layers[0].input, K.learning_phase()], [layer.output, ])
        activations = get_activations([X, 0])[0]
    except:
        # Ugly catch, a cleaner logic is welcome here.
        raise Exception("This layer cannot be plotted.")

    # For now we only handle feature map with 4 dimensions
    if activations.ndim != 4:
        raise Exception("Feature map of '{}' has {} dimensions which is not supported.".format(layer.name,
                                                                                               activations.ndim))

    # Set default matplotlib parameters
    if not 'interpolation' in kwargs.keys():
        kwargs['interpolation'] = "none"

    if not 'cmap' in kwargs.keys():
        kwargs['cmap'] = "gray"

    fig = plt.figure(figsize=(15, 15))

    # Compute nrows and ncols for images
    n_mosaic = len(activations)
    nrows = int(np.round(np.sqrt(n_mosaic)))
    ncols = int(nrows)
    if (nrows ** 2) < n_mosaic:
        ncols += 1

    # Compute nrows and ncols for mosaics
    if activations[0].shape[0] < n:
        n = activations[0].shape[0]

    nrows_inside_mosaic = int(np.round(np.sqrt(n)))
    ncols_inside_mosaic = int(nrows_inside_mosaic)

    if nrows_inside_mosaic ** 2 < n:
        ncols_inside_mosaic += 1

    for i, feature_map in enumerate(activations):
        mosaic = make_mosaic(feature_map[:n], nrows_inside_mosaic, ncols_inside_mosaic, border=1)

        ax = fig.add_subplot(nrows, ncols, i + 1)

        im = ax.imshow(mosaic, **kwargs)
        ax.set_title("Feature map #{} \nof layer#{} \ncalled '{}' \nof type {} ".format(i, layer_id,
                                                                                        layer.name,
                                                                                        layer.__class__.__name__))

        divider = make_axes_locatable(ax)
        cax = divider.append_axes("right", size="5%", pad=0.1)
        plt.colorbar(im, cax=cax)

    fig.tight_layout()
    return fig


def plot_all_feature_maps(model, X, n=256, ax=None, **kwargs):
    """
    """

    figs = []

    for i, layer in enumerate(model.layers):

        try:
            fig = plot_feature_map(model, i, X, n=n, ax=ax, **kwargs)
        except:
            pass
        else:
            figs.append(fig)

    return figs

# Plot all the weights when possible of the model
# The maximum number of filters per layer is n=256
# _ = plot_all_weights(model, n=256)

# Plot all the feature maps of the layer 2
# The maximum number of filters per feature maps is n=9

#print model.layers
#_ = plot_feature_map(model, 1, X_test[:6], n=9)
#_ = plot _all_feature_maps(model, X_test[:9], n=9)
#_=plot_weights(model, 0,n=9)

_=plot_all_weights(model, n=256)
plt.show()

